MoviePy - Writing audio in temp_audio.wav
chunk:   0%|          | 0/5995 [00:00<?, ?it/s, now=None]chunk:   7%|▋         | 408/5995 [00:00<00:01, 3941.61it/s, now=None]chunk:  13%|█▎        | 803/5995 [00:00<00:01, 3830.15it/s, now=None]chunk:  20%|█▉        | 1187/5995 [00:00<00:01, 3573.13it/s, now=None]chunk:  26%|██▌       | 1546/5995 [00:00<00:01, 3377.15it/s, now=None]chunk:  31%|███▏      | 1886/5995 [00:00<00:01, 3228.77it/s, now=None]chunk:  37%|███▋      | 2211/5995 [00:00<00:01, 3046.44it/s, now=None]chunk:  42%|████▏     | 2518/5995 [00:00<00:01, 2979.44it/s, now=None]chunk:  47%|████▋     | 2817/5995 [00:00<00:01, 2891.68it/s, now=None]chunk:  52%|█████▏    | 3107/5995 [00:00<00:01, 2867.00it/s, now=None]chunk:  57%|█████▋    | 3394/5995 [00:01<00:00, 2774.54it/s, now=None]chunk:  61%|██████▏   | 3672/5995 [00:01<00:00, 2704.10it/s, now=None]chunk:  66%|██████▋   | 3978/5995 [00:01<00:00, 2766.50it/s, now=None]chunk:  71%|███████   | 4255/5995 [00:01<00:00, 2763.43it/s, now=None]chunk:  76%|███████▌  | 4539/5995 [00:01<00:00, 2763.83it/s, now=None]chunk:  81%|████████  | 4834/5995 [00:01<00:00, 2817.94it/s, now=None]chunk:  86%|████████▌ | 5126/5995 [00:01<00:00, 2846.14it/s, now=None]chunk:  90%|█████████ | 5411/5995 [00:01<00:00, 2813.92it/s, now=None]chunk:  95%|█████████▌| 5699/5995 [00:01<00:00, 2832.90it/s, now=None]chunk: 100%|█████████▉| 5983/5995 [00:02<00:00, 2822.93it/s, now=None]                                                                      MoviePy - Done.
Audio extraction completed: assets/audios/C1_Cambridge_vid.wav

real	0m3,439s
user	0m4,301s
sys	0m0,477s
Audio device: cuda
Diarization has started: assets/audios/C1_Cambridge_vid.wav
Removing existing directory: cache/diarized_audios/C1_Cambridge_vid before creating a new one.
segmentation         ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00
speaker_counting     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00
embeddings           ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:06
discrete_diarization ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00
Segment duration longer than 29 seconds: 37631
Treshold: -50
Subsegment 0 saved from 0 to 22424 milliseconds. With a duration of 22424 milliseconds.
Subsegment 1 saved from 22424 to 37631 milliseconds. With a duration of 15207 milliseconds.
Segment 2 saved from 64020 to 65775 milliseconds. With a duration of 1755 milliseconds.
Segment 3 saved from 66062 to 82094 milliseconds. With a duration of 16032 milliseconds.
Segment 4 saved from 82617 to 91054 milliseconds. With a duration of 8437 milliseconds.
Segment 5 saved from 91341 to 114359 milliseconds. With a duration of 23018 milliseconds.
Segment 6 saved from 114578 to 120974 milliseconds. With a duration of 6396 milliseconds.
Segment 7 saved from 121497 to 126070 milliseconds. With a duration of 4573 milliseconds.
Segment 8 saved from 127150 to 143822 milliseconds. With a duration of 16672 milliseconds.
Segment 9 saved from 144143 to 154065 milliseconds. With a duration of 9922 milliseconds.
Segment 10 saved from 155112 to 165355 milliseconds. With a duration of 10243 milliseconds.
Segment 11 saved from 166300 to 189773 milliseconds. With a duration of 23473 milliseconds.
Segment 12 saved from 190245 to 194852 milliseconds. With a duration of 4607 milliseconds.
Segment 13 saved from 194987 to 196540 milliseconds. With a duration of 1553 milliseconds.
Segment 14 saved from 196793 to 206378 milliseconds. With a duration of 9585 milliseconds.
Segment 15 saved from 207154 to 208420 milliseconds. With a duration of 1266 milliseconds.
Segment 16 saved from 209196 to 219861 milliseconds. With a duration of 10665 milliseconds.
Segment 17 saved from 220536 to 235099 milliseconds. With a duration of 14563 milliseconds.
Segment 18 saved from 235690 to 236027 milliseconds. With a duration of 337 milliseconds.
Segment 19 saved from 236297 to 239858 milliseconds. With a duration of 3561 milliseconds.
Segment 20 saved from 240297 to 251451 milliseconds. With a duration of 11154 milliseconds.
Segment 21 saved from 251890 to 261323 milliseconds. With a duration of 9433 milliseconds.
Segment 22 saved from 261964 to 262420 milliseconds. With a duration of 456 milliseconds.
Segment 23 saved from 263213 to 264799 milliseconds. With a duration of 1586 milliseconds.
Diarization has completed.

real	0m16,188s
user	0m15,479s
sys	0m1,233s
/home/roman/miniconda3/envs/DeMINT/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/roman/miniconda3/envs/DeMINT/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, None], [2, 50360]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
############C1_Cambridge_vid
Starting to transcribe audio files of cache/diarized_audios/C1_Cambridge_vid and saving the transcript to ./cache/diarized_transcripts/C1_Cambridge_vid.json 
0_00.wav
now i'd like you to talk about something together for about two minutes here are some different methods of communication and a question for you to discuss first you have some time to look at the task
1_00.wav
now talk to each other about the advantages and disadvantages of these different methods of communication 
2_02.wav
 do you wanna stop or shall i?
3_01.wav
okay i'll start if you like so i think the most common way right now of communicating would be online chatting as everyone has mobile phones nowadays and we always have them on ourselves so it's really easy to just send a text message to someone and wait for them to answer it
4_02.wav
yes i agree but one of the main disadvantage of it is that you don't really know to whom you're talking to exactly so it's not that reliable
5_01.wav
yeah i agree so maybe a good alternative would be talking face to face you can it's like the most reliable way but one disadvantages it would have would be maybe you have to require the other person to be with you in the same place it cannot be in the other parts of the world like only chatting can do that
6_02.wav
yeah and and also you you can notice when someone's lying or when something's wrong between you two
7_01.wav
yeah so then maybe an alternative to that would be talking on the phone
8_02.wav
yes but sometimes it's not quite good it's not that effective because for example my grandmother doesn't hear too well so when when we talk on the phone she it's really difficult for her to to have a conversation with us
9_01.wav
so maybe using body language is something everyone understands so it may not be as easy as to communicate using body language if you lack the social skills maybe 
10_02.wav
um i agree i mean using body language is really difficult to to see when when someone's sad or when something's happening to them
11_01.wav
maybe written letters and postcards is the different one from all of all of them as it can also express emotions between countries and that excitement you have when you receive a postcard which is not really normal nowadays we all use email or chatting is really nice to have 
12_02.wav
yeah i mean the bad thing is that they sometimes can get lost
13_01.wav
 yeah i agree
14_00.wav
thank you now you have about a minute to decide which two methods of communication are the most difficult to carry out effectively 
15_01.wav
so which ones do you think
16_02.wav
um i will start off with using body language as we all as we've already said i mean it's really difficult to to notice and what do you think
17_01.wav
yeah also body languages it expresses a lot without us noticing or maybe if we talk using body language only like mimics do it's really difficult but it can express more than we think it can express
18_02.wav
 yeah
19_01.wav
then maybe talking face to face is really effective with communicating
20_02.wav
yeah that's really effective but one that wouldn't be that effective for example is written letters and postcards because as you said they can get lost and never get to where they had to
21_01.wav
yeah and they are really slow you have to wait for them to arrive and maybe even the message is outdated because it's something that happened before and doesn't have a point now
22_02.wav
 i will be
23_00.wav
thank you can i have the booklet please

real	0m52,248s
user	1m14,408s
sys	0m5,870s
Creating sorted sentence collection ...

real	0m2,074s
user	0m1,758s
sys	0m0,241s
/home/roman/miniconda3/envs/DeMINT/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/roman/miniconda3/envs/DeMINT/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/roman/miniconda3/envs/DeMINT/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Using T5
Orig: [1, 2, "i'd"], Cor: [1, 2, "I'd"], Type: 'R:ORTH'
Ignoring error type: R:ORTH
Orig: [6, 7, 'i?'], Cor: [6, 7, 'I?'], Type: 'R:ORTH'
Ignoring error type: R:ORTH
Orig: [0, 1, 'Okay'], Cor: [0, 1, 'Okay,'], Type: 'R:OTHER'
Ignoring error type: R:OTHER
Orig: [1, 2, "i'll"], Cor: [1, 2, "I'll"], Type: 'R:ORTH'
Ignoring error type: R:ORTH
Orig: [7, 8, 'i'], Cor: [7, 8, 'I'], Type: 'R:ORTH'
Ignoring error type: R:ORTH
Orig: [41, 53, 'a text message to someone and wait for them to answer it.'], Cor: [41, 41, ''], Type: 'U:OTHER'
Ignoring error type: U:OTHER
Orig: [0, 1, 'Yes'], Cor: [0, 1, 'Yes,'], Type: 'R:OTHER'
Ignoring error type: R:OTHER
Orig: [1, 2, 'i'], Cor: [1, 2, 'I'], Type: 'R:ORTH'
Ignoring error type: R:ORTH
Orig: [2, 3, 'agree'], Cor: [2, 3, 'agree,'], Type: 'R:OTHER'
Ignoring error type: R:OTHER
Orig: [8, 9, 'disadvantage'], Cor: [8, 9, 'disadvantages'], Type: 'R:NOUN:NUM'
Orig: [17, 18, 'to'], Cor: [17, 18, 'exactly'], Type: 'R:OTHER'
Ignoring error type: R:OTHER
Orig: [18, 19, 'whom'], Cor: [18, 19, 'who'], Type: 'R:PRON'
Orig: [21, 22, 'to'], Cor: [21, 22, 'to,'], Type: 'R:OTHER'
Ignoring error type: R:OTHER
Orig: [22, 23, 'exactly'], Cor: [22, 22, ''], Type: 'U:ADV'
Orig: [0, 1, 'Yeah'], Cor: [0, 1, 'Yeah,'], Type: 'R:OTHER'
Ignoring error type: R:OTHER
Orig: [1, 2, 'i'], Cor: [1, 2, 'I'], Type: 'R:ORTH'
Ignoring error type: R:ORTH
Orig: [13, 14, 'face'], Cor: [13, 14, 'face.'], Type: 'R:OTHER'
Ignoring error type: R:OTHER
Orig: [14, 17, "you can it's"], Cor: [14, 15, "It's"], Type: 'U:OTHER'
Ignoring error type: U:OTHER
Orig: [24, 25, 'disadvantages'], Cor: [22, 23, 'disadvantage'], Type: 'R:NOUN:NUM'
Orig: [46, 62, 'it cannot be in the other parts of the world like only chatting can do that.'], Cor: [44, 44, ''], Type: 'U:OTHER'
Ignoring error type: U:OTHER
Orig: [0, 1, 'Yeah'], Cor: [0, 1, 'Yeah,'], Type: 'R:OTHER'
Ignoring error type: R:OTHER
Orig: [1, 2, 'and'], Cor: [1, 1, ''], Type: 'U:CONJ'
Orig: [4, 5, 'you'], Cor: [3, 3, ''], Type: 'U:PRON'
Orig: [0, 1, 'Yeah'], Cor: [0, 1, 'Yeah,'], Type: 'R:OTHER'
Ignoring error type: R:OTHER
Orig: [1, 2, 'so'], Cor: [1, 1, ''], Type: 'U:ADV'
Orig: [0, 1, 'Yes'], Cor: [0, 1, 'Yes,'], Type: 'R:OTHER'
Ignoring error type: R:OTHER
Orig: [1, 2, 'but'], Cor: [1, 1, ''], Type: 'U:CONJ'
Orig: [6, 7, 'good'], Cor: [5, 6, 'good,'], Type: 'R:OTHER'
Ignoring error type: R:OTHER
Orig: [19, 20, 'well'], Cor: [18, 19, 'well,'], Type: 'R:OTHER'
Ignoring error type: R:OTHER
Orig: [21, 22, 'when'], Cor: [20, 20, ''], Type: 'U:ADV'
Orig: [28, 29, 'she'], Cor: [26, 26, ''], Type: 'U:PRON'
Orig: [34, 35, 'to'], Cor: [31, 31, ''], Type: 'U:VERB:FORM'
Orig: [40, 41, 'us.'], Cor: [36, 37, 'us'], Type: 'R:OTHER'
Ignoring error type: R:OTHER
Orig: [16, 17, 'as'], Cor: [16, 16, ''], Type: 'U:PREP'
Orig: [0, 1, 'Um'], Cor: [0, 1, 'Um,'], Type: 'R:OTHER'
Ignoring error type: R:OTHER
Orig: [1, 2, 'i'], Cor: [1, 2, 'I'], Type: 'R:ORTH'
Ignoring error type: R:ORTH
Orig: [3, 5, 'i mean'], Cor: [3, 4, 'that'], Type: 'R:OTHER'
Ignoring error type: R:OTHER
Orig: [11, 12, 'to'], Cor: [10, 10, ''], Type: 'U:VERB:FORM'
Orig: [14, 15, 'when'], Cor: [12, 12, ''], Type: 'U:ADV'
Orig: [5, 6, 'is'], Cor: [5, 6, 'are'], Type: 'R:VERB:SVA'
Orig: [10, 12, 'all of'], Cor: [10, 10, ''], Type: 'U:OTHER'
Ignoring error type: U:OTHER
Orig: [48, 50, 'to have.'], Cor: [46, 46, ''], Type: 'U:OTHER'
Ignoring error type: U:OTHER
Orig: [0, 1, 'Yeah'], Cor: [0, 1, 'Yeah,'], Type: 'R:OTHER'
Ignoring error type: R:OTHER
Orig: [1, 2, 'i'], Cor: [1, 2, 'I'], Type: 'R:ORTH'
Ignoring error type: R:ORTH
Orig: [0, 1, 'Yeah'], Cor: [0, 1, 'Yeah,'], Type: 'R:OTHER'
Ignoring error type: R:OTHER
Orig: [1, 2, 'i'], Cor: [1, 2, 'I'], Type: 'R:ORTH'
Ignoring error type: R:ORTH
Orig: [5, 6, 'think.'], Cor: [5, 6, 'think?'], Type: 'R:OTHER'
Ignoring error type: R:OTHER
Orig: [0, 1, 'Um'], Cor: [0, 1, 'Um,'], Type: 'R:OTHER'
Ignoring error type: R:OTHER
Orig: [1, 2, 'i'], Cor: [1, 2, 'I'], Type: 'R:ORTH'
Ignoring error type: R:ORTH
Orig: [11, 12, 'all'], Cor: [11, 12, 'all,'], Type: 'R:OTHER'
Ignoring error type: R:OTHER
Orig: [15, 16, 'said'], Cor: [15, 16, 'said,'], Type: 'R:OTHER'
Ignoring error type: R:OTHER
Orig: [16, 17, 'i'], Cor: [16, 16, ''], Type: 'U:PRON'
Orig: [21, 22, 'to'], Cor: [20, 20, ''], Type: 'U:VERB:FORM'
Orig: [28, 29, 'think.'], Cor: [26, 27, 'think?'], Type: 'R:OTHER'
Ignoring error type: R:OTHER
Orig: [9, 10, 'with'], Cor: [9, 10, 'in'], Type: 'R:PREP'
Orig: [0, 1, 'Yeah'], Cor: [0, 1, 'Yeah,'], Type: 'R:OTHER'
Ignoring error type: R:OTHER
Orig: [3, 4, 'effective'], Cor: [3, 4, 'effective,'], Type: 'R:ADJ'
Orig: [21, 22, 'said'], Cor: [21, 22, 'said,'], Type: 'R:VERB'
Orig: [1, 2, 'you'], Cor: [1, 2, 'you,'], Type: 'R:OTHER'
Ignoring error type: R:OTHER
Orig: [3, 4, 'i'], Cor: [3, 4, 'I'], Type: 'R:ORTH'
Ignoring error type: R:ORTH
Orig: [7, 8, 'please.'], Cor: [7, 8, 'please?'], Type: 'R:NOUN'

real	0m19,174s
user	0m19,485s
sys	0m0,928s
/home/roman/miniconda3/envs/DeMINT/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/roman/miniconda3/envs/DeMINT/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
['gpt-3.5-turbo-0125', 'gpt-4o-2024-05-13', 'gpt-4-turbo', 'gpt-4', 'gpt-4o', 'chatgpt-4o-latest', 'google/gemma-1.1-2b-it', 'google/gemma-1.1-7b-it', 'microsoft/Phi-3-mini-4k-instruct', 'meta-llama/Meta-Llama-3-8B-Instruct', 'meta-llama/Meta-Llama-3.1-8B-Instruct']
Model supported: meta-llama/Meta-Llama-3.1-8B-Instruct
Device detected:  cuda
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:  25%|██▌       | 1/4 [00:46<02:20, 46.70s/it]Downloading shards:  50%|█████     | 2/4 [01:33<01:33, 46.93s/it]Downloading shards:  75%|███████▌  | 3/4 [02:20<00:46, 46.74s/it]Downloading shards: 100%|██████████| 4/4 [02:31<00:00, 32.80s/it]Downloading shards: 100%|██████████| 4/4 [02:31<00:00, 37.93s/it]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:53<02:39, 53.22s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:07<01:00, 30.06s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:09<00:17, 17.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:10<00:00, 10.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:10<00:00, 17.56s/it]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.

Model loaded: meta-llama/Meta-Llama-3.1-8B-Instruct
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)

Type of model: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>

Type of tokenizer: <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>

Starting to explain sentences...
get_answers_batch, 6 batch, from LLM LLAMA meta-llama/Meta-Llama-3.1-8B-Instruct started
get_answers_batch from LLM finished. Time taken to get answers from LLM: 40.30197477340698 seconds
get_answers_batch, 6 batch, from LLM LLAMA meta-llama/Meta-Llama-3.1-8B-Instruct started
get_answers_batch from LLM finished. Time taken to get answers from LLM: 39.963531732559204 seconds
get_answers_batch, 6 batch, from LLM LLAMA meta-llama/Meta-Llama-3.1-8B-Instruct started
get_answers_batch from LLM finished. Time taken to get answers from LLM: 39.48364210128784 seconds
get_answers_batch, 6 batch, from LLM LLAMA meta-llama/Meta-Llama-3.1-8B-Instruct started
get_answers_batch from LLM finished. Time taken to get answers from LLM: 32.43111562728882 seconds
get_answers_batch, 6 batch, from LLM LLAMA meta-llama/Meta-Llama-3.1-8B-Instruct started
get_answers_batch from LLM finished. Time taken to get answers from LLM: 39.93720579147339 seconds
get_answers_batch, 6 batch, from LLM LLAMA meta-llama/Meta-Llama-3.1-8B-Instruct started
get_answers_batch from LLM finished. Time taken to get answers from LLM: 39.633050203323364 seconds
get_answers_batch, 3 batch, from LLM LLAMA meta-llama/Meta-Llama-3.1-8B-Instruct started
get_answers_batch from LLM finished. Time taken to get answers from LLM: 31.740620374679565 seconds
get_answers_batch, 3 batch, from LLM LLAMA meta-llama/Meta-Llama-3.1-8B-Instruct started
get_answers_batch from LLM finished. Time taken to get answers from LLM: 9.923416376113892 seconds
Model unloaded and GPU memory freed.
Time taken to explain sentences: 00:08:20

real	8m25,666s
user	5m31,408s
sys	1m14,404s
/home/roman/miniconda3/envs/DeMINT/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/roman/miniconda3/envs/DeMINT/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
MuPDF error: format error: No default Layer config

/home/roman/miniconda3/envs/DeMINT/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Preparing index...
Index ragatouille does exist. Skipping preparation index. If you want to reindex, delete the folder .ragatoille/colbert/indexes/tutor
Index prepared...
Processing sentence:  5
'RAGatouille INDEX' is None. Loading from index ./app/.ragatouille/colbert/indexes/tutor/.
'RAGatouille' exists. Searching..
Loading searcher for index tutor for the first time... This may take a few seconds
[Sep 13, 12:59:19] #> Loading codec...
[Sep 13, 12:59:20] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...
/home/roman/miniconda3/envs/DeMINT/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
[Sep 13, 13:00:23] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...
/home/roman/miniconda3/envs/DeMINT/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
[Sep 13, 13:01:12] #> Loading IVF...
[Sep 13, 13:01:12] #> Loading doclens...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 165.80it/s]
[Sep 13, 13:01:12] #> Loading codes and residuals...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  2.55it/s]100%|██████████| 1/1 [00:00<00:00,  2.55it/s]
Searcher loaded!

#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==
#> Input: . I'm happy to help you understand the error.

The error is an instance of the R:NOUN:NUM error type, which means there's a noun that's supposed to be a number, but is actually a number. The original sentence is "Yes i agree but one of the main disadvantage of it is that you don't really know to whom you're talking to so it's not that reliable.".

In this sentence, "one" is a number, but it's being used as a noun, which is incorrect. The corrected sentence is "Yes, I agree, but one of the main disadvantages of it is that you don't really know exactly who you're talking to, so it's not that reliable.".

The correction is that, 		 True, 		 None
#> Output IDs: torch.Size([153]), tensor([  101,     1,  1045,  1005,  1049,  3407,  2000,  2393,  2017,  3305,
         1996,  7561,  1012,  1996,  7561,  2003,  2019,  6013,  1997,  1996,
         1054,  1024, 15156,  1024, 16371,  2213,  7561,  2828,  1010,  2029,
         2965,  2045,  1005,  1055,  1037, 15156,  2008,  1005,  1055,  4011,
         2000,  2022,  1037,  2193,  1010,  2021,  2003,  2941,  1037,  2193,
         1012,  1996,  2434,  6251,  2003,  1000,  2748,  1045,  5993,  2021,
         2028,  1997,  1996,  2364, 20502,  1997,  2009,  2003,  2008,  2017,
         2123,  1005,  1056,  2428,  2113,  2000,  3183,  2017,  1005,  2128,
         3331,  2000,  2061,  2009,  1005,  1055,  2025,  2008, 10539,  1012,
         1000,  1012,  1999,  2023,  6251,  1010,  1000,  2028,  1000,  2003,
         1037,  2193,  1010,  2021,  2009,  1005,  1055,  2108,  2109,  2004,
         1037, 15156,  1010,  2029,  2003, 16542,  1012,  1996, 13371,  6251,
         2003,  1000,  2748,  1010,  1045,  5993,  1010,  2021,  2028,  1997,
         1996,  2364, 20502,  2015,  1997,  2009,  2003,  2008,  2017,  2123,
         1005,  1056,  2428,  2113,  3599,  2040,  2017,  1005,  2128,  3331,
         2000,  1010,   102], device='cuda:0')
#> Output Mask: torch.Size([153]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')

'RAGatouille' exists. Searching..
'RAGatouille' exists. Searching..
Processing sentence:  6
'RAGatouille' exists. Searching..
Processing sentence:  7
'RAGatouille' exists. Searching..
'RAGatouille' exists. Searching..
Processing sentence:  8
'RAGatouille' exists. Searching..
Processing sentence:  9
'RAGatouille' exists. Searching..
'RAGatouille' exists. Searching..
'RAGatouille' exists. Searching..
'RAGatouille' exists. Searching..
Processing sentence:  10
'RAGatouille' exists. Searching..
Processing sentence:  11
'RAGatouille' exists. Searching..
'RAGatouille' exists. Searching..
Processing sentence:  12
'RAGatouille' exists. Searching..
Processing sentence:  17
'RAGatouille' exists. Searching..
'RAGatouille' exists. Searching..
Processing sentence:  20
'RAGatouille' exists. Searching..
Processing sentence:  21
'RAGatouille' exists. Searching..
'RAGatouille' exists. Searching..
Processing sentence:  24
'RAGatouille' exists. Searching..

real	2m41,712s
user	2m22,933s
sys	0m13,473s
589.77user 96.62system 12:40.51elapsed 90%CPU (0avgtext+0avgdata 9778240maxresident)k
14071256inputs+32615504outputs (1630major+11328373minor)pagefaults 0swaps
